{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e755b0-12a9-4694-8b6e-2a826e0a3d26",
   "metadata": {},
   "source": [
    "### 모델학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4afc92e-d2d6-41bd-9932-8e3cd6dc2b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9e72b9-72a4-4fd4-9236-c3b9c193ede5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\82105\\anaconda3\\lib\\site-packages (25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in c:\\users\\82105\\anaconda3\\lib\\site-packages (2.8.0+cpu)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torchvision-0.23.0%2Bcpu-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torchaudio-2.8.0%2Bcpu-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\82105\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\82105\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\82105\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\82105\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\82105\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\82105\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\82105\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\82105\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\82105\\anaconda3\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\82105\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\82105\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Using cached https://download.pytorch.org/whl/cpu/torchvision-0.23.0%2Bcpu-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "Using cached https://download.pytorch.org/whl/cpu/torchaudio-2.8.0%2Bcpu-cp312-cp312-win_amd64.whl (2.5 MB)\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   ---------------------------------------- 2/2 [torchaudio]\n",
      "\n",
      "Successfully installed torchaudio-2.8.0+cpu torchvision-0.23.0+cpu\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e6cece56-073a-4b7a-be22-80a07e2ce1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "14fc2662-c20e-40af-bd91-085338052555",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")   #cuda장비인지 GPU장비인지 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8a6116b4-bd51-4e34-8c6c-b95efa8e1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d7233b49-af51-4a20-b742-df22cdda3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_base = transforms.Compose([transforms.Resize((64,64)),transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "42756725-95eb-408f-a178-ff3d15785b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(root='./splitted/train',transform=transform_base)\n",
    "val_dataset = ImageFolder(root='./splitted/val',transform=transform_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "edb8a4f2-64de-4e55-b055-b57bb74c6ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "722f23e6-8b8a-4da6-b856-dee1fdf65c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d2f3d2-ba47-4548-9c65-6a2bacfcaba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4d769-f9f1-417d-afed-f3ba0a4404aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af56dd-1351-47bb-9ec9-4e61c4c63c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bad3020-e788-43f4-92f3-1ae2e58e0877",
   "metadata": {},
   "source": [
    "### 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "391b2e4c-5323-42bd-968f-534245883a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "009a4ce7-7061-4c02-a6a0-d5ace9dec3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "\n",
    "        self.fcl = nn.Linear(4096, 512)\n",
    "        self.fc2 = nn.Linear(512, 33)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        x = x.view(-1,4096)\n",
    "        x = self.fcl(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model_base = Net().to(DEVICE)\n",
    "optimizer = optim.Adam(model_base.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d526d8e-ceaf-4c7f-a033-b556e3d95115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d0ede1e-0643-4ca7-b749-5852d2a8134b",
   "metadata": {},
   "source": [
    "#### 모델 학습을 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6f260e69-3db1-4cde-85b3-700026ae0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f23abc-77ff-4d21-b458-20f8adf9905f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54b73b6d-ad6e-4d8d-be60-d2377de085e7",
   "metadata": {},
   "source": [
    "#### 모델 평가를 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a31f6fe2-3033-457d-8eb3-338e813403e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "\n",
    "\n",
    "            test_loss += F.cross_entropy(output, \n",
    "                                        target, reduction='sum').item()\n",
    "\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct +=pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2370ab38-88e1-4fb7-ac3b-15e46bd42ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba54a1b-bec7-4a3c-92b6-d34291bb5171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043401c6-640e-4ab6-8347-588a252e50c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83206ada-eb04-4faf-9534-ce70ed947816",
   "metadata": {},
   "source": [
    "### 모델 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e49669c4-760b-480e-91bb-c73f253d8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "89e2ffb9-42a1-4e3f-bd5d-ed79b1fe98cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------epoch1-----------\n",
      "train Loss : 0.9949, Accuracy: 70.58%\n",
      "val Loss : 1.0345, Accuracy: 69.27%\n",
      "completed in 3m 46s\n",
      "--------------epoch2-----------\n",
      "train Loss : 0.7425, Accuracy: 76.85%\n",
      "val Loss : 0.7880, Accuracy: 75.85%\n",
      "completed in 3m 39s\n",
      "--------------epoch3-----------\n",
      "train Loss : 0.5632, Accuracy: 82.31%\n",
      "val Loss : 0.6227, Accuracy: 80.57%\n",
      "completed in 3m 40s\n",
      "--------------epoch4-----------\n",
      "train Loss : 0.4796, Accuracy: 85.54%\n",
      "val Loss : 0.5432, Accuracy: 83.61%\n",
      "completed in 3m 42s\n",
      "--------------epoch5-----------\n",
      "train Loss : 0.4043, Accuracy: 87.77%\n",
      "val Loss : 0.4793, Accuracy: 85.27%\n",
      "completed in 3m 57s\n",
      "--------------epoch6-----------\n",
      "train Loss : 0.3682, Accuracy: 88.45%\n",
      "val Loss : 0.4527, Accuracy: 85.67%\n",
      "completed in 3m 40s\n",
      "--------------epoch7-----------\n",
      "train Loss : 0.3301, Accuracy: 90.22%\n",
      "val Loss : 0.4123, Accuracy: 87.08%\n",
      "completed in 3m 53s\n",
      "--------------epoch8-----------\n",
      "train Loss : 0.2777, Accuracy: 91.55%\n",
      "val Loss : 0.3688, Accuracy: 88.31%\n",
      "completed in 3m 59s\n",
      "--------------epoch9-----------\n",
      "train Loss : 0.2515, Accuracy: 92.20%\n",
      "val Loss : 0.3490, Accuracy: 88.62%\n",
      "completed in 4m 4s\n",
      "--------------epoch10-----------\n",
      "train Loss : 0.2296, Accuracy: 92.92%\n",
      "val Loss : 0.3362, Accuracy: 88.90%\n",
      "completed in 3m 43s\n",
      "--------------epoch11-----------\n",
      "train Loss : 0.2059, Accuracy: 93.75%\n",
      "val Loss : 0.3185, Accuracy: 89.80%\n",
      "completed in 3m 38s\n",
      "--------------epoch12-----------\n",
      "train Loss : 0.1773, Accuracy: 94.88%\n",
      "val Loss : 0.2887, Accuracy: 90.96%\n",
      "completed in 3m 45s\n",
      "--------------epoch13-----------\n",
      "train Loss : 0.1887, Accuracy: 94.31%\n",
      "val Loss : 0.3068, Accuracy: 89.90%\n",
      "completed in 3m 44s\n",
      "--------------epoch14-----------\n",
      "train Loss : 0.1657, Accuracy: 95.41%\n",
      "val Loss : 0.2886, Accuracy: 90.95%\n",
      "completed in 3m 40s\n",
      "--------------epoch15-----------\n",
      "train Loss : 0.1532, Accuracy: 95.58%\n",
      "val Loss : 0.2874, Accuracy: 90.84%\n",
      "completed in 4m 16s\n",
      "--------------epoch16-----------\n",
      "train Loss : 0.1350, Accuracy: 96.10%\n",
      "val Loss : 0.2694, Accuracy: 91.44%\n",
      "completed in 4m 41s\n",
      "--------------epoch17-----------\n",
      "train Loss : 0.1366, Accuracy: 95.95%\n",
      "val Loss : 0.2757, Accuracy: 91.11%\n",
      "completed in 4m 39s\n",
      "--------------epoch18-----------\n",
      "train Loss : 0.0991, Accuracy: 97.46%\n",
      "val Loss : 0.2418, Accuracy: 92.44%\n",
      "completed in 4m 41s\n",
      "--------------epoch19-----------\n",
      "train Loss : 0.1056, Accuracy: 97.00%\n",
      "val Loss : 0.2453, Accuracy: 92.04%\n",
      "completed in 4m 7s\n",
      "--------------epoch20-----------\n",
      "train Loss : 0.1005, Accuracy: 97.10%\n",
      "val Loss : 0.2520, Accuracy: 92.18%\n",
      "completed in 4m 29s\n",
      "--------------epoch21-----------\n",
      "train Loss : 0.0829, Accuracy: 97.89%\n",
      "val Loss : 0.2365, Accuracy: 92.77%\n",
      "completed in 4m 19s\n",
      "--------------epoch22-----------\n",
      "train Loss : 0.0955, Accuracy: 97.48%\n",
      "val Loss : 0.2475, Accuracy: 92.25%\n",
      "completed in 4m 21s\n",
      "--------------epoch23-----------\n",
      "train Loss : 0.0764, Accuracy: 97.99%\n",
      "val Loss : 0.2300, Accuracy: 92.73%\n",
      "completed in 4m 29s\n",
      "--------------epoch24-----------\n",
      "train Loss : 0.0832, Accuracy: 97.87%\n",
      "val Loss : 0.2385, Accuracy: 92.70%\n",
      "completed in 4m 11s\n",
      "--------------epoch25-----------\n",
      "train Loss : 0.0667, Accuracy: 98.27%\n",
      "val Loss : 0.2277, Accuracy: 92.85%\n",
      "completed in 4m 10s\n",
      "--------------epoch26-----------\n",
      "train Loss : 0.0746, Accuracy: 98.03%\n",
      "val Loss : 0.2397, Accuracy: 92.34%\n",
      "completed in 6m 11s\n",
      "--------------epoch27-----------\n",
      "train Loss : 0.0746, Accuracy: 97.89%\n",
      "val Loss : 0.2470, Accuracy: 92.39%\n",
      "completed in 3m 49s\n",
      "--------------epoch28-----------\n",
      "train Loss : 0.0577, Accuracy: 98.54%\n",
      "val Loss : 0.2191, Accuracy: 93.15%\n",
      "completed in 3m 33s\n",
      "--------------epoch29-----------\n",
      "train Loss : 0.0531, Accuracy: 98.90%\n",
      "val Loss : 0.2072, Accuracy: 93.39%\n",
      "completed in 3m 56s\n",
      "--------------epoch30-----------\n",
      "train Loss : 0.0428, Accuracy: 99.08%\n",
      "val Loss : 0.2110, Accuracy: 93.64%\n",
      "completed in 4m 11s\n"
     ]
    }
   ],
   "source": [
    "def train_baseline(model, train_loader, val_loader,\n",
    "                  optimizer, num_epochs = 30):\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        since = time.time()\n",
    "        train(model, train_loader, optimizer)\n",
    "        train_loss, train_acc = evaluate(model, train_loader)\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('--------------epoch{}-----------'.format(epoch))\n",
    "\n",
    "        print('train Loss : {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))\n",
    "\n",
    "        print('val Loss : {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
    "\n",
    "        print('completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "\n",
    "base = train_baseline(model_base, train_loader, val_loader, optimizer, EPOCH)\n",
    "\n",
    "torch.save(base,'baseline.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63431554-f663-4540-9317-98108f824a78",
   "metadata": {},
   "source": [
    "### 모델평가를 위한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "65c5e9de-e6f6-4a7a-9623-3d5d6a998f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_base = transforms.Compose([transforms.Resize([64,64]),\n",
    "                                    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7530ac3d-3566-4105-8427-c0b062021bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_base = ImageFolder(root='./splitted/test',transform=transform_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3b7d967b-2cbf-4d7e-a5ab-53036e97c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_base,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809950ef-cbf1-485f-8863-7f14718e1a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73180e89-0d55-437e-a33c-4ccf7451c641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c536684-b4de-4208-8101-c3875f41e776",
   "metadata": {},
   "source": [
    "### 모델성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7e8dc034-48aa-4a1c-aaaf-851a29a50a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fcl): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=33, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.serialization import add_safe_globals\n",
    "add_safe_globals([Net])   # 신뢰하는 클래스로 Net 등록\n",
    "\n",
    "baseline = torch.load('baseline.pt', map_location=DEVICE, weights_only=False)\n",
    "baseline.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043348d2-375b-4acf-af4a-ae949747728f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "43604c27-d70e-4040-80e6-a23faad60cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss,test_accuracy = evaluate(baseline, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "438032d9-3da0-4458-ac68-97db7ea1ea25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline test acc:  94.02929027412692\n"
     ]
    }
   ],
   "source": [
    "print('baseline test acc: ',test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a16e8e-0cf4-4255-8fb0-5e6d22f7eb80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
